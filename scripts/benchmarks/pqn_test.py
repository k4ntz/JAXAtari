"""
Adapted from https://github.com/mttga/purejaxql/blob/main/purejaxql/pqn_gymnax.py
"""

import copy
import os
from struct import unpack
import threading
import time
import jax
import jax.numpy as jnp
import numpy as np
from functools import partial
from typing import Any
import os
from typing import Dict, Union

import chex
import optax
import flax.linen as nn
from flax.training.train_state import TrainState
import jaxatari.environment
from jaxatari.environment import EnvState
from jaxatari.wrappers import AtariWrapper, PixelObsWrapper, FlattenObservationWrapper, LogWrapper, ObjectCentricWrapper, NormalizeObservationWrapper
import hydra
from omegaconf import OmegaConf

from safetensors.flax import save_file, load_file
from flax.traverse_util import flatten_dict, unflatten_dict

import jaxatari
import wandb

def video_callback(states, dones, renderer, mod=False):
    """
    Starting a new thread to render video so that it doesn't block training.
    """
    video_renderer(states, dones, renderer, mod)

def video_renderer(states, dones, renderer, mod):
    print("Rendering video...")
    video_folder = f"{wandb.run.dir}/media/videos/"
    os.makedirs(video_folder, exist_ok=True)

    if hasattr(states, 'atari_state'):
        states = states.atari_state
    if hasattr(states, 'env_state'):
        states = states.env_state

    # num_states is where the first done is True
    num_states = jnp.argmax(dones)
    # or len of the first array of the states pytree
    if num_states == 0:
        num_states = len(states[-1])

    # select every 4th frame (and only the first num_states)
    states_reduced = jax.tree_util.tree_map(lambda x: x[:num_states], states)
    rasters = jax.vmap(renderer.render)(states_reduced)
    frames = np.array(rasters, dtype=np.uint8)
    # shape currently is (N, W, H, 3)
    # but should be (N, 3, W, H)
    frames = np.transpose(frames, (0, 3, 1, 2))

    fps = 30 
    video = wandb.Video(frames, fps=fps, format="mp4")
    name = "video"
    if mod:
        name = "video_mod"
    # wandb.log({name: video}, step=wandb.run.step)
    wandb.log({name: video})
    print("Video done.")

def save_params(params: Dict, filename: Union[str, os.PathLike]) -> None:
    flattened_dict = flatten_dict(params, sep=',')
    save_file(flattened_dict, filename)

def load_params(filename:Union[str, os.PathLike]) -> Dict:
    flattened_dict = load_file(filename)
    return unflatten_dict(flattened_dict, sep=",")

class CNN(nn.Module):

    norm_type: str = "layer_norm"

    @nn.compact
    def __call__(self, x: jnp.ndarray, train: bool):
        if self.norm_type == "layer_norm":
            normalize = lambda x: nn.LayerNorm()(x)
        elif self.norm_type == "batch_norm":
            normalize = lambda x: nn.BatchNorm(use_running_average=not train)(x)
        else:
            normalize = lambda x: x
        x = nn.Conv(
            32,
            kernel_size=(8, 8),
            strides=(4, 4),
            padding="VALID",
            kernel_init=nn.initializers.he_normal(),
        )(x)
        x = normalize(x)
        x = nn.relu(x)
        x = nn.Conv(
            64,
            kernel_size=(4, 4),
            strides=(2, 2),
            padding="VALID",
            kernel_init=nn.initializers.he_normal(),
        )(x)
        x = normalize(x)
        x = nn.relu(x)
        x = nn.Conv(
            64,
            kernel_size=(3, 3),
            strides=(1, 1),
            padding="VALID",
            kernel_init=nn.initializers.he_normal(),
        )(x)
        x = normalize(x)
        x = nn.relu(x)
        x = x.reshape((x.shape[0], -1))
        x = nn.Dense(512, kernel_init=nn.initializers.he_normal())(x)
        x = normalize(x)
        x = nn.relu(x)
        return x

class QNetwork(nn.Module):
    action_dim: int
    hidden_size: int = 128
    num_layers: int = 2
    norm_type: str = "layer_norm"
    norm_input: bool = False
    object_centric: bool = True

    @nn.compact
    def __call__(self, x: jnp.ndarray, train: bool):
        if self.norm_input:
            x = nn.BatchNorm(use_running_average=not train)(x)
        else:
            # dummy normalize input for global compatibility
            x_dummy = nn.BatchNorm(use_running_average=not train)(x)

        if self.object_centric:
            # Use MLP for object centric observations
            if self.norm_type == "layer_norm":
                normalize = lambda x: nn.LayerNorm()(x)
            elif self.norm_type == "batch_norm":
                normalize = lambda x: nn.BatchNorm(use_running_average=not train)(x)
            else:
                normalize = lambda x: x

            for l in range(self.num_layers):
                x = nn.Dense(self.hidden_size)(x)
                x = normalize(x)
                x = nn.relu(x)
        else:
            # Use CNN for pixel based observations
            x = CNN(norm_type=self.norm_type)(x, train)

        x = nn.Dense(self.action_dim)(x)

        return x


@chex.dataclass(frozen=True)
class Transition:
    obs: chex.Array
    action: chex.Array
    reward: chex.Array
    done: chex.Array
    next_obs: chex.Array
    q_val: chex.Array


class CustomTrainState(TrainState):
    batch_stats: Any
    timesteps: int = 0
    n_updates: int = 0
    grad_steps: int = 0


def make_test(config, save_params, batch_stats):

    config["NUM_UPDATES"] = (
        config["TOTAL_TIMESTEPS"] // config["NUM_STEPS"] // config["NUM_ENVS"]
    )

    config["NUM_UPDATES_DECAY"] = (
        config["TOTAL_TIMESTEPS_DECAY"] // config["NUM_STEPS"] // config["NUM_ENVS"]
    )

    assert (config["NUM_STEPS"] * config["NUM_ENVS"]) % config[
        "NUM_MINIBATCHES"
    ] == 0, "NUM_MINIBATCHES must divide NUM_STEPS*NUM_ENVS"

    env = jaxatari.make(config["ENV_NAME"].lower())
    mod_env = env
    if config.get("MOD_NAME", None) is not None:
        mod_env = jaxatari.modify(env, config.get("ENV_NAME", None).lower(), config.get("MOD_NAME", None).lower())
    renderer = jaxatari.make_renderer(config["ENV_NAME"].lower())

    def apply_wrappers(env):
        env = AtariWrapper(env, episodic_life=True, frame_skip=4, frame_stack_size=4, sticky_actions=True, max_pooling=True, clip_reward=True, noop_reset=30)
        if config.get("OBJECT_CENTRIC", False):
            env = ObjectCentricWrapper(env)
            env = FlattenObservationWrapper(env)
        else:
            env = PixelObsWrapper(env)
        env = NormalizeObservationWrapper(env)
        env = LogWrapper(env)
        return env

    env = apply_wrappers(env)
    mod_env = apply_wrappers(mod_env)

    # epsilon-greedy exploration
    def eps_greedy_exploration(rng, q_vals, eps):
        rng_a, rng_e = jax.random.split(
            rng
        )  # a key for sampling random actions and one for picking
        greedy_actions = jnp.argmax(q_vals, axis=-1)
        chosed_actions = jnp.where(
            jax.random.uniform(rng_e, greedy_actions.shape)
            < eps,  # pick the actions that should be random
            jax.random.randint(
                rng_a, shape=greedy_actions.shape, minval=0, maxval=q_vals.shape[-1]
            ),  # sample random actions,
            greedy_actions,
        )
        return chosed_actions

    def test(rng):

        original_rng = rng[0]

        eps_scheduler = optax.linear_schedule(
            config["EPS_START"],
            config["EPS_FINISH"],
            (config["EPS_DECAY"]) * config["NUM_UPDATES_DECAY"],
        )

        lr_scheduler = optax.linear_schedule(
            init_value=config["LR"],
            end_value=1e-20,
            transition_steps=(config["NUM_UPDATES_DECAY"])
            * config["NUM_MINIBATCHES"]
            * config["NUM_EPOCHS"],
        )
        lr = lr_scheduler if config.get("LR_LINEAR_DECAY", False) else config["LR"]

        # INIT NETWORK AND OPTIMIZER
        network = QNetwork(
            action_dim=env.action_space().n,
            hidden_size=config.get("HIDDEN_SIZE", 128),
            num_layers=config.get("NUM_LAYERS", 2),
            norm_type=config["NORM_TYPE"],
            norm_input=config.get("NORM_INPUT", False),
            object_centric=config.get("OBJECT_CENTRIC", True),
        )

        def create_agent(rng):
            init_x = jnp.zeros((1, *env.observation_space().shape))
            network_variables = network.init(rng, init_x, train=False)
            tx = optax.chain(
                optax.clip_by_global_norm(config["MAX_GRAD_NORM"]),
                optax.radam(learning_rate=lr),
            )
            params = network_variables["params"] if save_params is None else save_params
            batch_sts = network_variables["batch_stats"] if batch_stats is None else batch_stats
            batch_sts = jax.tree.map(lambda x: x.squeeze(), batch_sts)  # remove vmap dim
            params = jax.tree.map(lambda x: x.squeeze(), params)  # remove vmap dim
            train_state = CustomTrainState.create(
                apply_fn=network.apply,
                params=params,
                batch_stats=batch_sts,
                tx=tx,
            )
            return train_state

        rng, _rng = jax.random.split(rng)
        train_state = create_agent(rng)
        
        @partial(jax.jit, static_argnums=(1,))
        def get_test_metrics(train_state, mod, rng):

            def _env_step(carry, _):
                env_state, last_obs, rng = carry
                rng, _rng = jax.random.split(rng)
                q_vals = network.apply(
                    {
                        "params": train_state.params,
                        "batch_stats": train_state.batch_stats,
                    },
                    last_obs,
                    train=False,
                )
                eps = jnp.full(config["TEST_NUM_ENVS"], config["EPS_TEST"])
                action = jax.vmap(eps_greedy_exploration)(
                    jax.random.split(_rng, config["TEST_NUM_ENVS"]), q_vals, eps
                )
                if mod:
                    new_obs, new_env_state, reward, done, info = jax.vmap(mod_env.step)(env_state, action)
                else:
                    new_obs, new_env_state, reward, done, info = jax.vmap(env.step)(env_state, action)
                env_state_vid = jax.tree.map(lambda x: x[0], new_env_state)
                dones_vid = jax.tree.map(lambda x: x[0], done)
                return (new_env_state, new_obs, rng), (info, env_state_vid, dones_vid)

            rng, _rng = jax.random.split(rng)
            reset_keys = jax.random.split(_rng, config["TEST_NUM_ENVS"])
            if mod:
                init_obs, env_state = jax.vmap(mod_env.reset)(reset_keys)
            else:
                init_obs, env_state = jax.vmap(env.reset)(reset_keys)

            _, output = jax.lax.scan(
                _env_step, (env_state, init_obs, _rng), None, config["TEST_NUM_STEPS"]
            )
            infos, env_states, dones = output[0], output[1], output[2]

            if config.get("RECORD_VIDEO", False):
                jax.debug.callback(video_callback, env_states, dones, renderer, mod=mod),

            # return mean of done infos
            done_infos = jax.tree_util.tree_map(
                lambda x: jnp.nanmean(
                    jnp.where(
                        infos["returned_episode"],
                        x.squeeze(),
                        jnp.nan,
                    )
                ),
                infos,
            )
            return done_infos

        rng, _rng = jax.random.split(rng)
        test_metrics = get_test_metrics(train_state, False, _rng)

        mod_metrics = get_test_metrics(train_state, True, _rng) if config.get("MOD_NAME", None) is not None else {}

        return {"test_metrics": test_metrics, "mod_metrics": mod_metrics}

    return test

def single_run(config):

    config = {**config, **config["alg"]}

    alg_name = config.get("ALG_NAME", "pqn")
    env_name = config["ENV_NAME"]
    oc = "oc" if config.get("OBJECT_CENTRIC", False) else "pixel"

    rng = jax.random.PRNGKey(config["SEED"])
    rngs = jax.random.split(rng, config["NUM_SEEDS"])

    save_dir = os.path.join(config["SAVE_PATH"], env_name)
    train_state_params = []
    batch_stats = []
    for i, rng in enumerate(rngs):
        save_path = os.path.join(
            save_dir,
            f'{alg_name}_{env_name}_{oc}_seed{config["SEED"]}_vmap{i}.safetensors',
        )
        if not os.path.exists(save_path):
            raise ValueError(f"Save path {save_path} does not exist!")

        params = load_params(save_path)  # test loading
        batch_sts = load_params(save_path.replace(".safetensors", "_bs.safetensors"))
        train_state_params.append(params)
        batch_stats.append(batch_sts)
    train_state_params = jax.tree_util.tree_map(
        lambda *xs: jnp.stack(xs), *train_state_params
    )
    batch_stats = jax.tree_util.tree_map(
        lambda *xs: jnp.stack(xs), *batch_stats
    )

    wandb.init(
        entity=config["ENTITY"],
        project=config["PROJECT"],
        tags=[
            alg_name.upper(),
            env_name.upper(),
            f"jax_{jax.__version__}",
        ],
        name=config.get("NAME", f'{config["ALG_NAME"]}_{config["ENV_NAME"]}_TEST'),
        config=config,
        mode=config["WANDB_MODE"],
    )


    t0 = time.time()

    train_vjit = jax.jit(jax.vmap(make_test(config, train_state_params, batch_stats)))
    train_vjit.lower(rngs).compile()
    compile_time = time.time()
    print(f"Compile time: {compile_time - t0} seconds.")
    outs = jax.block_until_ready(train_vjit(rngs))
    print(f"Run time: {time.time() - compile_time} seconds.")
    print(f"Total: {time.time()-t0} seconds.")
    avg_return = outs["test_metrics"]["returned_episode_returns"]
    avg_return_mod = outs["mod_metrics"]["returned_episode_returns"]
    avg_len = outs["test_metrics"]["returned_episode_lengths"]
    avg_len_mod = outs["mod_metrics"]["returned_episode_lengths"]
    print(f"Average return of default env: {avg_return}, length: {avg_len}.")
    if config.get("MOD_NAME", None) is not None:
        print(f"Average return of modified env ({config['MOD_NAME']}): {avg_return_mod}, length: {avg_len_mod}.")

    


@hydra.main(version_base=None, config_path="./config", config_name="config")
def main(config):
    config = OmegaConf.to_container(config)
    print("Config:\n", OmegaConf.to_yaml(config))
    single_run(config)


if __name__ == "__main__":
    main()