# This is adapted from Atari configuration 
NAME_INITIALS: "RE"
ALG_NAME: "pqn"
TOTAL_TIMESTEPS: 5e7 # 1e8 == 100M, but times 4 (frame-skipping) -> pqn paper: enough to outperform Rainbow
TOTAL_TIMESTEPS_DECAY: 5e7 # will be used for decay functions, in case you want to test for less timesteps and keep decays same
NUM_ENVS: 128 # parallel environments
NUM_STEPS: 32 # steps per environment in each update
# NUM_ENVS / NUM_STEPS should be 8 for original PQN (they did 128/32)
EPS_START: 1.0
EPS_FINISH: 0.001
EPS_DECAY: 0.1
NUM_EPOCHS: 2 
NUM_MINIBATCHES: 32 # minibatches per epoch
NORM_TYPE: "layer_norm" # layer_norm or batch_norm
LR: 2.5e-4
MAX_GRAD_NORM: 10
LR_LINEAR_DECAY: False 
GAMMA: 0.99
LAMBDA: 0.65

# env specific
ENV_NAME: "Pong"
ENV_KWARGS: {}
OBJECT_CENTRIC: False
# Apply this mod during testing
# Activate this only for the test run (or debugging)
# Set to None for final training run.
MOD_NAME: LazyEnemyWrapper

# Note: This will massively increase compile and run time!
# Turn off for final reported training run.
TEST_DURING_TRAINING: False
TEST_INTERVAL: 0.01 # in terms of total updates
TEST_NUM_ENVS: 128
TEST_NUM_STEPS: 10000
EPS_TEST: 0.0 # 0 for greedy policy
RECORD_VIDEO: True 