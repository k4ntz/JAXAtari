# This is adapted from Atari configuration 
NAME_INITIALS: "RE"
ALG_NAME: "pqn"
TOTAL_TIMESTEPS: 5e7 # 1e8 == 100M, but times 4 (frame-skipping) -> pqn paper: enough to outperform Rainbow
TOTAL_TIMESTEPS_DECAY: 5e7 # will be used for decay functions, in case you want to test for less timesteps and keep decays same
NUM_ENVS: 1024 #512 # parallel environments
NUM_STEPS: 128 # steps per environment in each update
# NUM_ENVS / NUM_STEPS should be 8 for original PQN (they did 128/32)
EPS_START: 1.0
EPS_FINISH: 0.001
EPS_DECAY: 0.1
NUM_EPOCHS: 5 #2 -> adapted UtD ratio
NUM_MINIBATCHES: 128 # minibatches per epoch
NORM_TYPE: "layer_norm" # layer_norm or batch_norm
LR: 1.0e-4
MAX_GRAD_NORM: 10
LR_LINEAR_DECAY: False #True
GAMMA: 0.99
LAMBDA: 0.65

# env specific
ENV_NAME: "Pong"
ENV_KWARGS: {}
OBJECT_CENTRIC: True
# Apply this mod during testing
# Set to None for training run.
# Activate this for the final test run (or debugging)
MOD_NAME: LazyEnemyWrapper

# Note: This will massively increase compile and run time!
# But is useful for debugging (watching the video).
# Turn off for reported training run.
TEST_DURING_TRAINING: False
TEST_INTERVAL: 0.1 # in terms of total updates
TEST_NUM_ENVS: 1024 #512
TEST_NUM_STEPS: 10000
EPS_TEST: 0.0 # 0 for greedy policy
RECORD_VIDEO: True 